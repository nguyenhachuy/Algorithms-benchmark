{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statistics import mean\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADULT_PATH = './adult/adult.pkl'\n",
    "DOTA_PATH = './dota2Dataset/dota2Train.pkl'\n",
    "CONNECT_4_PATH = './connect-4/connect-4.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n",
      "(50, 108) (50, 115) (50, 42)\n",
      "[-1  1] [-1  1] [ 1 -1]\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "\n",
    "adult = pickle.load(open(ADULT_PATH, 'rb'))\n",
    "dota = pickle.load(open(DOTA_PATH, 'rb'))\n",
    "connect = pickle.load(open(CONNECT_4_PATH, 'rb'))\n",
    "\n",
    "#test\n",
    "adult = adult.iloc[:10000, :]\n",
    "dota = dota.iloc[:10000, :]\n",
    "connect = connect.iloc[:10000, :]\n",
    "\n",
    "adult_label = adult.iloc[:,0]\n",
    "dota_label = dota.iloc[:,0]\n",
    "connect_label = connect.iloc[:,0]\n",
    "\n",
    "adult.drop(columns='label', inplace=True)\n",
    "dota.drop(columns='label', inplace=True)\n",
    "connect.drop(columns='label', inplace=True)\n",
    "\n",
    "adult = pd.get_dummies(adult)\n",
    "dota = pd.get_dummies(dota)\n",
    "connect = pd.get_dummies(connect)\n",
    "\n",
    "print(type(adult), type(dota), type(connect))\n",
    "print(adult.shape, dota.shape, connect.shape)\n",
    "print(adult_label.unique(), dota_label.unique(), connect_label.unique())\n",
    "\n",
    "datasets = [adult.values, dota.values, connect.values]\n",
    "labels = [adult_label.values, dota_label.values, connect_label.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition schemes and classifiers\n",
    "partitions = [0.2, 0.5, 0.8]\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=1024, max_depth=10, random_state=0, n_jobs=-1)\n",
    "max_features = [1,2,4,6,8,12,16,20]\n",
    "parameters_1 = {'clf__max_features': max_features}\n",
    "\n",
    "lgc = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "penalty = ['l1', 'l2']\n",
    "C = [10**x for x in range(-8,5)]\n",
    "parameters_2 = {'clf__penalty': penalty, 'clf__C': C}\n",
    "\n",
    "svc = SVC(gamma='auto')\n",
    "C = [10**x for x in range(-7,3)]\n",
    "kernel = ['linear', 'poly']\n",
    "degree = [2,3]\n",
    "parameters_3 = {'clf__C': C, 'clf__kernel': kernel, 'clf__degree': degree}\n",
    "\n",
    "clfs = [\n",
    "    RandomForestClassifier(n_estimators=1024, max_depth=20, random_state=0, n_jobs=-1), \n",
    "    LogisticRegression(random_state=0, n_jobs=-1),\n",
    "    SVC(gamma='auto')\n",
    "]\n",
    "\n",
    "parameters = [parameters_1, parameters_2, parameters_3]\n",
    "clf_names = ['rf', 'lg', 'svm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0, dataset 0, partition 0.2, trial 0\n",
      "Accuracy: 0.9\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=20, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 20}\n",
      "Average test accuracy for rf, 0, 0.2: 0.9\n",
      "Average train accuracy for rf, 0, 0.2: 1.0\n",
      "Average val accuracy for rf, 0, 0.2: 0.7490842490842491\n",
      "Classifier 0, dataset 0, partition 0.5, trial 0\n",
      "Accuracy: 0.72\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=4, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 4}\n",
      "Average test accuracy for rf, 0, 0.5: 0.72\n",
      "Average train accuracy for rf, 0, 0.5: 1.0\n",
      "Average val accuracy for rf, 0, 0.5: 0.7962962962962963\n",
      "Classifier 0, dataset 0, partition 0.8, trial 0\n",
      "Accuracy: 0.75\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 1}\n",
      "Average test accuracy for rf, 0, 0.8: 0.75\n",
      "Average train accuracy for rf, 0, 0.8: 1.0\n",
      "Average val accuracy for rf, 0, 0.8: 0.8333333333333334\n",
      "Classifier 0, dataset 1, partition 0.2, trial 0\n",
      "Accuracy: 0.4\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=6, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 6}\n",
      "Average test accuracy for rf, 1, 0.2: 0.4\n",
      "Average train accuracy for rf, 1, 0.2: 1.0\n",
      "Average val accuracy for rf, 1, 0.2: 0.4246031746031746\n",
      "Classifier 0, dataset 1, partition 0.5, trial 0\n",
      "Accuracy: 0.36\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 1}\n",
      "Average test accuracy for rf, 1, 0.5: 0.36\n",
      "Average train accuracy for rf, 1, 0.5: 1.0\n",
      "Average val accuracy for rf, 1, 0.5: 0.3611111111111111\n",
      "Classifier 0, dataset 1, partition 0.8, trial 0\n",
      "Accuracy: 0.5\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 1}\n",
      "Average test accuracy for rf, 1, 0.8: 0.5\n",
      "Average train accuracy for rf, 1, 0.8: 1.0\n",
      "Average val accuracy for rf, 1, 0.8: 0.6666666666666666\n",
      "Classifier 0, dataset 2, partition 0.2, trial 0\n",
      "Accuracy: 0.8\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 1}\n",
      "Average test accuracy for rf, 2, 0.2: 0.8\n",
      "Average train accuracy for rf, 2, 0.2: 1.0\n",
      "Average val accuracy for rf, 2, 0.2: 0.9010989010989011\n",
      "Classifier 0, dataset 2, partition 0.5, trial 0\n",
      "Accuracy: 0.92\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=2, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 2}\n",
      "Average test accuracy for rf, 2, 0.5: 0.92\n",
      "Average train accuracy for rf, 2, 0.5: 1.0\n",
      "Average val accuracy for rf, 2, 0.5: 0.8425925925925926\n",
      "Classifier 0, dataset 2, partition 0.8, trial 0\n",
      "Accuracy: 0.85\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=1, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1024, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__max_features': 1}\n",
      "Average test accuracy for rf, 2, 0.8: 0.85\n",
      "Average train accuracy for rf, 2, 0.8: 1.0\n",
      "Average val accuracy for rf, 2, 0.8: 0.9166666666666666\n",
      "Classifier 1, dataset 0, partition 0.2, trial 0\n",
      "Accuracy: 0.8\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l1', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 1e-08, 'clf__penalty': 'l1'}\n",
      "Average test accuracy for lg, 0, 0.2: 0.8\n",
      "Average train accuracy for lg, 0, 0.2: 0.7502374169040836\n",
      "Average val accuracy for lg, 0, 0.2: 0.7509157509157509\n",
      "Classifier 1, dataset 0, partition 0.5, trial 0\n",
      "Accuracy: 0.76\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l1', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 1e-08, 'clf__penalty': 'l1'}\n",
      "Average test accuracy for lg, 0, 0.5: 0.76\n",
      "Average train accuracy for lg, 0, 0.5: 0.7598039215686274\n",
      "Average val accuracy for lg, 0, 0.5: 0.7592592592592592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 1, dataset 0, partition 0.8, trial 0\n",
      "Accuracy: 0.75\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l1', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 1e-08, 'clf__penalty': 'l1'}\n",
      "Average test accuracy for lg, 0, 0.8: 0.75\n",
      "Average train accuracy for lg, 0, 0.8: 0.8055555555555557\n",
      "Average val accuracy for lg, 0, 0.8: 0.8333333333333334\n",
      "Classifier 1, dataset 1, partition 0.2, trial 0\n",
      "Accuracy: 0.6\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l1', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 1, 'clf__penalty': 'l1'}\n",
      "Average test accuracy for lg, 1, 0.2: 0.6\n",
      "Average train accuracy for lg, 1, 0.2: 0.7985347985347985\n",
      "Average val accuracy for lg, 1, 0.2: 0.5277777777777778\n",
      "Classifier 1, dataset 1, partition 0.5, trial 0\n",
      "Accuracy: 0.52\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l2', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 10000, 'clf__penalty': 'l2'}\n",
      "Average test accuracy for lg, 1, 0.5: 0.52\n",
      "Average train accuracy for lg, 1, 0.5: 1.0\n",
      "Average val accuracy for lg, 1, 0.5: 0.7175925925925926\n",
      "Classifier 1, dataset 1, partition 0.8, trial 0\n",
      "Accuracy: 0.5\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l1', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 1e-08, 'clf__penalty': 'l1'}\n",
      "Average test accuracy for lg, 1, 0.8: 0.5\n",
      "Average train accuracy for lg, 1, 0.8: 0.5\n",
      "Average val accuracy for lg, 1, 0.8: 0.5\n",
      "Classifier 1, dataset 2, partition 0.2, trial 0\n",
      "Accuracy: 0.8\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l2', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 10, 'clf__penalty': 'l2'}\n",
      "Average test accuracy for lg, 2, 0.2: 0.8\n",
      "Average train accuracy for lg, 2, 0.2: 0.9377967711301044\n",
      "Average val accuracy for lg, 2, 0.2: 0.9249084249084248\n",
      "Classifier 1, dataset 2, partition 0.5, trial 0\n",
      "Accuracy: 0.84\n",
      "Best estimator: Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
      "          penalty='l2', random_state=0, solver='warn', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best params:  {'clf__C': 1e-08, 'clf__penalty': 'l2'}\n",
      "Average test accuracy for lg, 2, 0.5: 0.84\n",
      "Average train accuracy for lg, 2, 0.5: 0.8799019607843137\n",
      "Average val accuracy for lg, 2, 0.5: 0.8796296296296297\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 267, in fit\n    self._final_estimator.fit(Xt, y, **fit_params)\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\", line 1302, in fit\n    sample_weight=sample_weight)\n  File \"C:\\Users\\Gus\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\", line 872, in _fit_liblinear\n    \" class: %r\" % classes_[0])\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f9b5050072ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 clf = GridSearchCV(estimator=pipeline, param_grid=parameters[classifier], \n\u001b[0;32m     18\u001b[0m                                    n_jobs=-1, cv=3, return_train_score=True, iid=False)\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classifier {}, dataset {}, partition {}, trial {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "for classifier in range(3):\n",
    "    for dataset in range(3):        \n",
    "        for partition in partitions:\n",
    "            test_accs = []\n",
    "            train_accs = []\n",
    "            val_accs = []\n",
    "            for trial in range(3):\n",
    "                X = datasets[dataset]\n",
    "                y = labels[dataset]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=partition, stratify=y)\n",
    "                \n",
    "                #Make pipeline\n",
    "                pipeline = Pipeline(\n",
    "                    [('scaler', MinMaxScaler()),\n",
    "                     ('clf', clfs[classifier]),                     \n",
    "                ])\n",
    "                clf = GridSearchCV(estimator=pipeline, param_grid=parameters[classifier], \n",
    "                                   n_jobs=-1, cv=3, return_train_score=True, iid=False)\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                print('Classifier {}, dataset {}, partition {}, trial {}'.format(classifier, dataset, partition, trial))\n",
    "                \n",
    "                train_accuracy = clf.cv_results_['mean_train_score'][clf.best_index_]\n",
    "                train_accs.append(train_accuracy)\n",
    "                \n",
    "                val_accuracy = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "                val_accs.append(val_accuracy)\n",
    "                \n",
    "                test_accuracy = clf.score(X_test, y_test)\n",
    "                test_accs.append(test_accuracy)\n",
    "                \n",
    "                print('Accuracy: {}'.format(test_accuracy))\n",
    "\n",
    "                print('Best estimator:', clf.best_estimator_)\n",
    "            \n",
    "                print('Best params: ', clf.best_params_)\n",
    "            \n",
    "                clf_name = clf_names[classifier]\n",
    "                dirname = \"./classifier/{}/{}/{}/{}\".format(\n",
    "                    classifier, dataset, partition, trial\n",
    "                )\n",
    "               \n",
    "                if not os.path.exists(dirname):\n",
    "                    os.makedirs(dirname)\n",
    "                    \n",
    "                clf_dump = open(\"{}/{}.pkl\".format(\n",
    "                    dirname, clf_name\n",
    "                ),\"wb\")\n",
    "                pickle.dump(clf, clf_dump)\n",
    "                clf_dump.close()\n",
    "                \n",
    "                score_dump = open(\"{}/y_test.pkl\".format(\n",
    "                    dirname\n",
    "                ),\"wb\")\n",
    "                pickle.dump(y_test, score_dump)\n",
    "                score_dump.close()\n",
    "                \n",
    "            avg_test = mean(test_accs)\n",
    "            avg_train = mean(train_accs)\n",
    "            avg_val = mean(val_accs)\n",
    "            \n",
    "            test_accs.clear()\n",
    "            train_accs.clear()\n",
    "            val_accs.clear()\n",
    "            \n",
    "            print('Average test accuracy for {}, {}, {}: {}'.format(\n",
    "                clf_names[classifier], \n",
    "                dataset,\n",
    "                partition,\n",
    "                avg_test\n",
    "            ))\n",
    "            \n",
    "            print('Average train accuracy for {}, {}, {}: {}'.format(\n",
    "                clf_names[classifier], \n",
    "                dataset,\n",
    "                partition,\n",
    "                avg_train\n",
    "            ))\n",
    "            \n",
    "            print('Average val accuracy for {}, {}, {}: {}'.format(\n",
    "                clf_names[classifier], \n",
    "                dataset,\n",
    "                partition,\n",
    "                avg_val\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
